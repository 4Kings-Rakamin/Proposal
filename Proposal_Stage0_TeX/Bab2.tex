\newpage
\section{Tinjauan Pustaka}
\subsection{Proses Rekrutmen}
Proses rekrutmen adalah serangkaian langkah yang diambil oleh perusahaan untuk menarik, menilai, dan memilih kandidat yang paling sesuai untuk mengisi posisi yang tersedia. Proses ini biasanya dimulai dengan identifikasi kebutuhan tenaga kerja, diikuti oleh pencarian kandidat melalui berbagai saluran seperti iklan lowongan kerja, agen perekrutan, dan media sosial. Setelah kandidat ditemukan, mereka akan melalui tahap seleksi yang melibatkan penilaian kualifikasi, wawancara, dan tes keterampilan. Akhirnya, keputusan perekrutan dibuat berdasarkan evaluasi menyeluruh dari semua kandidat yang telah melalui proses seleksi. Proses rekrutmen yang efektif tidak hanya memastikan bahwa perusahaan mendapatkan karyawan yang berkualitas, tetapi juga membantu dalam membangun budaya organisasi yang positif dan mendukung tujuan bisnis jangka panjang. \parencite{mathis2017human}

Adapun beberapa faktor yang mempengaruhi keputusan perekrutan meliputi:
\begin{enumerate}
    \item Demografis: usia, gender, latar belakang pendidikan. \parencite{ng2005person}
    \item Pengalaman kerja: jumlah tahun pengalaman dan variasi perusahaan sebelumnya. \parencite{ployhart2006staffing}
    \item Kompetensi teknis dan soft skills: hasil tes keterampilan (skill score), wawancara, dan penilaian kepribadian. \parencite{schmidt1998validity}
    \item Faktor eksternal: jarak tempat tinggal ke kantor sering menjadi pertimbangan dalam retensi. \parencite{hausknecht2009targeted}
    \item Strategi rekrutmen: pendekatan organisasi (job fairs, rekrutmen online, campus hiring) dapat memengaruhi kualitas kandidat. \parencite{breaugh2013employee}.
\end{enumerate}

Berdasarkan studi literatur, faktor-faktor di atas secara signifikan mempengaruhi keputusan perekrutan. Dimana hal tersebut dapat menjadi pertimbangan penting dalam pengembangan model prediktif untuk proses perekrutan.

\subsection{CRISP-DM}
CRISP-DM (Cross-Industry Standard Process for Data Mining) adalah metodologi standar yang digunakan dalam proyek data mining dan analisis data. Metodologi ini terdiri dari enam fase utama yang membantu dalam mengorganisir dan mengelola proyek data secara sistematis. \parencite{chumbar2020crispdm}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{Gambar/CRISP-DM.png}
    \caption{Alur Kerja CRISP-DM}
    \label{fig:crisp-dm}
\end{figure}

Gambar \ref{fig:crisp-dm} menggambarkan alur kerja CRISP-DM yang terdiri dari enam fase utama, yaitu:
\begin{enumerate}
    \item Business Understanding: Memahami tujuan bisnis dan kebutuhan proyek.
    \item Data Understanding: Mengumpulkan dan memahami data yang tersedia.
    \item Data Preparation: Membersihkan dan mempersiapkan data untuk analisis.
    \item Modeling: Membangun model prediktif menggunakan teknik machine learning.
    \item Evaluation: Mengevaluasi model untuk memastikan bahwa tujuan bisnis tercapai.
    \item Deployment: Menerapkan model dalam lingkungan produksi untuk digunakan dalam pengambilan keputusan bisnis.
\end{enumerate}

\subsection{EDA (Exploratory Data Analysis)}
Exploratory Data Analysis (EDA) adalah proses awal dalam analisis data yang bertujuan untuk memahami karakteristik dan pola dalam dataset. EDA melibatkan berbagai teknik visualisasi dan statistik untuk mengeksplorasi data, mengidentifikasi outlier, dan menemukan hubungan antara variabel. Proses ini sangat penting karena membantu dalam mengarahkan langkah-langkah selanjutnya dalam analisis data, seperti pemilihan fitur dan pemodelan. \parencite{tukey1977exploratory}

\subsection{T-Test}
T-Test adalah metode statistik yang digunakan untuk membandingkan rata-rata dari dua kelompok data. Uji ini membantu menentukan apakah perbedaan antara kedua kelompok tersebut signifikan secara statistik atau hanya terjadi secara kebetulan. T-Test dapat digunakan dalam berbagai konteks, seperti membandingkan hasil tes antara dua kelompok siswa atau mengevaluasi efektivitas dua metode pengajaran yang berbeda. Hasil dari uji T-Test memberikan nilai p-value yang digunakan untuk menilai signifikansi perbedaan antara kedua kelompok. \parencite{deveaux2011intro}

%rumus t-test
Rumus untuk menghitung nilai T-Test ($t$) adalah sebagai berikut:
\begin{equation}
    t = \frac{\bar{X_1} - \bar{X_2}}{\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}}
\end{equation}
Dimana:
\begin{itemize}
    \item $t$ = nilai T-Test
    \item $\bar{X_1}$ = rata-rata kelompok pertama
    \item $\bar{X_2}$ = rata-rata kelompok kedua
    \item $s_1^2$ = varians kelompok pertama
    \item $s_2^2$ = varians kelompok kedua
    \item $n_1$ = ukuran sampel kelompok pertama
    \item $n_2$ = ukuran sampel kelompok kedua
\end{itemize}

Singkatnya uji t-test ini memudahkan kita dalam membandingkan dua kelompok data untuk menentukan apakah ada perbedaan yang signifikan antara keduanya. Apabila p-value lebih kecil dari tingkat signifikansi (misalnya 0,05), maka kita menolak hipotesis nol dan menyimpulkan bahwa ada perbedaan yang signifikan antara kedua kelompok tersebut. Ini berguna apabila fitur numerik ingin dibandingkan terhadap target kategorikal.


\subsection{Chi Square Test}
Chi Square Test adalah metode statistik yang digunakan untuk menguji hubungan antara dua variabel kategorikal. Uji ini membandingkan frekuensi yang diamati dalam setiap kategori dengan frekuensi yang diharapkan jika tidak ada hubungan antara variabel. Hasil dari uji Chi Square memberikan nilai p-value yang digunakan untuk menentukan apakah hubungan antara variabel tersebut signifikan secara statistik. \parencite{agresti2018statistical}
% \parencite{agresti2018statistical}

%rumus uji chi square
Rumus untuk menghitung nilai Chi Square ($\chi^2$) adalah sebagai berikut:
\begin{equation}
    \chi^2 = \sum \frac{(O_i - E_i)^2}{E_i}
\end{equation}
Dimana:
\begin{itemize}
    \item $\chi^2$ = nilai Chi Square
    \item $O_i$ = frekuensi yang diamati dalam kategori ke-i
    \item $E_i$ = frekuensi yang diharapkan dalam kategori ke-i
    \item $\sum$ = penjumlahan untuk semua kategori
\end{itemize}

Apabila p-value lebih kecil dari tingkat signifikansi (misalnya 0,05), maka kita menolak hipotesis nol dan menyimpulkan bahwa ada hubungan yang signifikan antara kedua variabel tersebut. Ini berguna apabila fitur kategorikal ingin dibandingkan terhadap target kategorikal.
%\parencite{agresti2018statistical}

\subsection{Standard Scaler}
Standard Scaler adalah teknik normalisasi data yang digunakan untuk mengubah fitur numerik sehingga memiliki rata-rata (mean) nol dan standar deviasi (standard deviation) satu. Proses ini membantu dalam mengurangi skala variabilitas antar fitur, sehingga model machine learning dapat belajar lebih efektif. Standard Scaler sangat berguna ketika fitur-fitur dalam dataset memiliki rentang nilai yang berbeda-beda, karena dapat meningkatkan konvergensi dan kinerja model. \parencite{jain2016feature}
%\parencite{jain2016feature}
Rumus untuk menghitung nilai yang telah dinormalisasi ($z$) menggunakan Standard Scaler adalah sebagai berikut:

\begin{equation}
    z = \frac{(X - \mu)}{\sigma}
\end{equation}
Dimana:
\begin{itemize}
    \item $z$ = nilai yang telah dinormalisasi
    \item $X$ = nilai asli dari fitur
    \item $\mu$ = rata-rata (mean) dari fitur
    \item $\sigma$ = standar deviasi (standard deviation) dari fitur
    \item $X - \mu$ = selisih antara nilai asli dan rata-rata
    \item $\frac{(X - \mu)}{\sigma}$ = hasil pembagian selisih dengan standar deviasi
\end{itemize}

Dengan menggunakan Standard Scaler, setiap fitur numerik dalam dataset akan diubah sehingga memiliki distribusi yang seragam, yang pada gilirannya dapat meningkatkan performa model machine learning.

\subsection{Logistic Regression}
Logistic Regression adalah metode statistik yang digunakan untuk memodelkan hubungan antara satu atau lebih variabel independen (fitur) dengan variabel dependen biner (target). Metode ini digunakan untuk memprediksi probabilitas kejadian suatu peristiwa, seperti apakah seorang kandidat akan diterima atau ditolak dalam proses perekrutan. Logistic Regression menggunakan fungsi logit untuk mengubah output linier menjadi probabilitas yang berada dalam rentang 0 hingga 1. \parencite{hosmer2013applied}
%\parencite{hosmer2013applied}
Rumus untuk menghitung probabilitas ($p$) menggunakan Logistic Regression adalah sebagai berikut:
\begin{equation}
    p = \frac{1}{1 + e^{-(\beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_nX_n)}}
\end{equation}

Dimana:
\begin{itemize}
    \item $p$ = probabilitas kejadian (misalnya, kandidat diterima)
    \item $e$ = basis dari logaritma natural (sekitar 2,718)
    \item $\beta_0$ = intercept (konstanta)
    \item $\beta_1, \beta_2, ..., \beta_n$ = koefisien regresi untuk masing-masing fitur
    \item $X_1, X_2, ..., X_n$ = nilai dari fitur-fitur independen
    \item $\beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_nX_n$ = kombinasi linier dari fitur-fitur
\end{itemize}

Logistic Regression sangat berguna dalam berbagai aplikasi, termasuk analisis risiko kredit, diagnosis medis, dan prediksi perilaku konsumen. Dengan kemampuannya untuk memberikan interpretasi yang jelas melalui koefisien regresi, metode ini memungkinkan pemahaman yang lebih baik tentang faktor-faktor yang mempengaruhi keputusan biner.

\subsection{Decision Tree}
Decision Tree adalah algoritma machine learning yang digunakan untuk klasifikasi dan regresi. Algoritma ini bekerja dengan membagi data menjadi subset berdasarkan fitur-fitur tertentu, sehingga membentuk struktur pohon yang terdiri dari simpul (nodes) dan cabang (branches). Setiap simpul mewakili keputusan berdasarkan nilai fitur, sementara cabang menghubungkan simpul-simpul tersebut. Proses pembagian data berlanjut hingga mencapai simpul daun (leaf nodes) yang memberikan prediksi akhir. Decision Tree sangat populer karena kemampuannya untuk menangani data kategorikal dan numerik, serta memberikan interpretasi yang mudah dipahami.

Rumus untuk menghitung impurity menggunakan Gini Index ($G$) adalah sebagai berikut:

\begin{equation}
    G = 1 - \sum_{i=1}^{C} p_i^2
\end{equation}

Dimana:
\begin{itemize}
    \item $G$ = nilai Gini Index
    \item $C$ = jumlah kelas dalam target
    \item $p_i$ = proporsi dari kelas ke-i dalam subset data
    \item $\sum_{i=1}^{C} p_i^2$ = penjumlahan kuadrat proporsi untuk semua kelas
    \item $1 - \sum_{i=1}^{C} p_i^2$ = hasil pengurangan dari 1 dengan penjumlahan kuadrat proporsi
\end{itemize}

Decision Tree sangat berguna dalam berbagai aplikasi, termasuk diagnosis medis, analisis risiko kredit, dan prediksi perilaku konsumen. Dengan kemampuannya untuk memberikan interpretasi yang jelas melalui struktur pohon, metode ini memungkinkan pemahaman yang lebih baik tentang faktor-faktor yang mempengaruhi keputusan klasifikasi.

\subsection{Random Forest}
Random Forest adalah algoritma ensemble learning yang digunakan untuk klasifikasi dan regresi. Algoritma ini bekerja dengan membangun sejumlah besar pohon keputusan (decision trees) secara acak dan menggabungkan hasil prediksi dari masing-masing pohon untuk menghasilkan prediksi akhir yang lebih akurat dan stabil. Random Forest mengurangi risiko overfitting yang sering terjadi pada pohon keputusan tunggal dengan cara mengambil rata-rata (untuk regresi) atau mode (untuk klasifikasi) dari hasil prediksi semua pohon dalam hutan. \parencite{breiman2001random}
%\parencite{breiman2001random}

Rumus untuk menghitung prediksi akhir ($\hat{y}$) dalam Random Forest adalah sebagai berikut:

\begin{equation}
    \hat{y} = \frac{1}{T} \sum_{t=1}^{T} h_t(X)
\end{equation}
Dimana:
\begin{itemize}
    \item $\hat{y}$ = prediksi akhir dari Random Forest
    \item $T$ = jumlah pohon keputusan dalam hutan
    \item $h_t(X)$ = prediksi dari pohon keputusan ke-t untuk input $X$
    \item $\sum_{t=1}^{T} h_t(X)$ = penjumlahan prediksi dari semua pohon keputusan
    \item $\frac{1}{T} \sum_{t=1}^{T} h_t(X)$ = hasil pembagian penjumlahan dengan jumlah pohon untuk mendapatkan rata-rata prediksi
\end{itemize}

Random Forest sangat berguna dalam berbagai aplikasi, termasuk diagnosis medis, analisis risiko kredit, dan prediksi perilaku konsumen. Dengan kemampuannya untuk menangani data yang kompleks dan memberikan interpretasi yang jelas melalui fitur penting (feature importance), metode ini memungkinkan pemahaman yang lebih baik tentang faktor-faktor yang mempengaruhi keputusan klasifikasi atau regresi.

\subsection{XGBoost}
XGBoost (Extreme Gradient Boosting) adalah algoritma machine learning yang digunakan untuk klasifikasi dan regresi. Algoritma ini merupakan implementasi dari teknik boosting yang menggabungkan beberapa model lemah (weak learners), biasanya pohon keputusan, untuk membentuk model yang lebih kuat dan akurat. XGBoost dikenal karena kemampuannya dalam menangani data besar dan kompleks, serta memberikan performa yang tinggi melalui optimasi paralel dan regularisasi.

Rumus untuk menghitung prediksi akhir ($\hat{y}$) dalam XGBoost adalah sebagai berikut:
\begin{equation}
    \hat{y} = \sum_{k=1}^{K} f_k(X)
\end{equation}

Dimana:
\begin{itemize}
    \item $\hat{y}$ = prediksi akhir dari XGBoost
    \item $K$ = jumlah model lemah (pohon keputusan) yang digabungkan
    \item $f_k(X)$ = prediksi dari model lemah ke-k untuk input $X$
    \item $\sum_{k=1}^{K} f_k(X)$ = penjumlahan prediksi dari semua model lemah
\end{itemize}

XGBoost sangat berguna dalam berbagai aplikasi, termasuk diagnosis medis, analisis risiko kredit, dan prediksi perilaku konsumen. Dengan kemampuannya untuk menangani data yang kompleks dan memberikan interpretasi yang jelas melalui fitur penting (feature importance), metode ini memungkinkan pemahaman yang lebih baik tentang faktor-faktor yang mempengaruhi keputusan klasifikasi atau regresi.




